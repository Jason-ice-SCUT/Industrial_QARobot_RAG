import sys
from pathlib import Path
project_root = str(Path(__file__).parent.parent)
sys.path.append(project_root)
from GradioApp import Gradio_env

import torch
from datasets import load_from_disk
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    Trainer
)
from peft import LoraConfig, get_peft_model


# ============================= 1. é…ç½®å‚æ•° ==================================
MODEL_NAME = "Qwen/Qwen2-0.5B" 
DATASET_PATH = "Finetune_datasets/ft_dataset.arrow"  # æ•°æ®é›†
OUTPUT_DIR = "Finetune_datasets/lora_qwen_industrial_safety"  # å¾®è°ƒç»“æœä¿å­˜ç›®å½•
LORA_R = 4  # LoRA ç§©ï¼ˆè¶Šå¤§æ•ˆæœå¯èƒ½è¶Šå¥½ï¼Œä½†æ˜¾å­˜å ç”¨è¶Šé«˜ï¼‰
LORA_ALPHA = 8  # ç¼©æ”¾å› å­ï¼ˆé€šå¸¸æ˜¯ R çš„ 2 å€ï¼‰
LORA_DROPOUT = 0.1  # LoRA  dropout ç‡
BATCH_SIZE = 2  # æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼Œ16GB æ˜¾å­˜æ¨è 2-4ï¼‰
GRADIENT_ACCUMULATION_STEPS = 8  # æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡ï¼‰
LEARNING_RATE = 1.5e-4  # å­¦ä¹ ç‡ï¼ˆLoRA å¾®è°ƒæ¨è 1e-4 ~ 3e-4ï¼‰
MAX_SEQ_LENGTH = 256  # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆæ ¹æ®æ•°æ®é•¿åº¦è°ƒæ•´ï¼‰
EPOCHS = 3  # è®­ç»ƒè½®æ¬¡ï¼ˆå°æ•°æ®é›† 3-5 è½®è¶³å¤Ÿï¼‰

# ============================ 2. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›† ===========================
def load_and_preprocess_dataset(dataset_path, tokenizer):
    # åŠ è½½æœ¬åœ°æ•°æ®é›†
    dataset = load_from_disk(dataset_path)
    print(f"æ•°æ®é›†ç»“æ„ï¼š{dataset}")
    
    # æ ¼å¼åŒ–æ•°æ®ä¸º Qwen å¯¹è¯æ¨¡æ¿ï¼ˆinstruction + input -> outputï¼‰
    def format_example(instruction, input_text, output_text):
        return f"""<|im_start|>system
{instruction}<|im_end|>
<|im_start|>user
{input_text}<|im_end|>
<|im_start|>assistant
{output_text}<|im_end|>"""
    
  # Tokenize å‡½æ•° (ä¿®å¤äº† labels é€»è¾‘)
    def tokenize_function(examples):
        instructions = examples["instruction"]
        input_texts = examples["input"]
        output_texts = examples["output"]
        
        texts = [format_example(inst, inp, out) for inst, inp, out in zip(instructions, input_texts, output_texts)]
        
        # è¿™é‡ŒåŒæ—¶ç”Ÿæˆ input_ids å’Œ attention_mask
        tokenized = tokenizer(
            texts,
            truncation=True,
            max_length=MAX_SEQ_LENGTH,
            padding="max_length",
            return_tensors="pt"
        )
        
        # åˆ›å»º labelsï¼Œé»˜è®¤å¤åˆ¶ input_ids
        input_ids = tokenized["input_ids"]
        labels = input_ids.clone()
        
        # å°† padding éƒ¨åˆ†çš„ label è®¾ç½®ä¸º -100 (è¿™æ ·è®¡ç®— loss æ—¶ä¼šè¢«å¿½ç•¥)
        # æ³¨æ„ï¼štokenizer.pad_token_id å¯¹äº Qwen å¯èƒ½æ˜¯ Noneï¼Œæ‰€ä»¥æˆ‘ä»¬è¦ç”¨ eos_token_id æˆ–è€…æ‰‹åŠ¨æŒ‡å®šçš„ pad_token
        pad_token_id = tokenizer.pad_token_id
        labels[labels == pad_token_id] = -100
        
        # å°† labels æ”¾å› tokenized å­—å…¸ä¸­
        tokenized["labels"] = labels
       
        
        return tokenized
    
    # ç§»é™¤æ—§åˆ—ï¼Œé˜²æ­¢æ•°æ®å†—ä½™
    column_names = dataset["train"].column_names if "train" in dataset else dataset.column_names
    
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=column_names,
        num_proc=1 # CPU è¿è¡Œæ—¶ï¼Œå¦‚æœä¸ç¨³å®šå¯ä»¥è®¾ä¸º 1
    )
    
    return tokenized_dataset

# ====================== 3. é…ç½®æ¨¡å‹ï¼ˆ4-bit é‡åŒ–èŠ‚çœæ˜¾å­˜ï¼‰===========================
def load_model_and_tokenizer():
    # åŠ è½½ Tokenizerï¼ˆQwen2 ä¸“ç”¨ï¼‰
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        padding_side="right" # å³å¯¹é½è¡¥å…¨
    )
    tokenizer.pad_token = tokenizer.eos_token  # Qwen2 é»˜è®¤æ—  pad_tokenï¼Œéœ€æ‰‹åŠ¨è®¾ç½®
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map="cpu",  # å¼ºåˆ¶ä½¿ç”¨ CPU
        trust_remote_code=True,
        torch_dtype=torch.float32,  
        load_in_8bit=False,  
        load_in_4bit=False,
        low_cpu_mem_usage=True  # å¯ç”¨ä½å†…å­˜æ¨¡å¼
    )
    
    # å…³é—­ç¼“å­˜å’Œå¼ é‡å¹¶è¡Œ
    model.config.use_cache = False
    model.config.pretraining_tp = 1
    return model, tokenizer

# =============================== 4. é…ç½® LoRA =====================================
def setup_lora(model):
    lora_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        target_modules=["q_proj", "v_proj"],  # Qwen æ¨¡å‹çš„ç›®æ ‡å±‚ï¼ˆæ³¨æ„åŠ›+æŠ•å½±å±‚ï¼‰
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type="CAUSAL_LM"  # å› æœè¯­è¨€æ¨¡å‹ä»»åŠ¡
    )
    
    # åº”ç”¨ LoRA åˆ°æ¨¡å‹
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()  # æ‰“å°å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹ï¼ˆé€šå¸¸ <1%ï¼‰
    return model

# ================================ 5. è®­ç»ƒé…ç½® ==================================
def get_training_args():
    return TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        learning_rate=LEARNING_RATE,
        num_train_epochs=EPOCHS,
        logging_steps=1,
        save_strategy="epoch",
        evaluation_strategy="epoch",
        fp16=False,
        report_to="none",
        seed=42,
        load_best_model_at_end=True,
        no_cuda=True,
        disable_tqdm=False,
        # ä»…ä¿ç•™æ—§ç‰ˆ transformers å…¼å®¹çš„å‚æ•°
        optim="adamw_torch",  # ä½¿ç”¨ torch åŸç”Ÿ AdamWï¼Œé¿å… accelerate åŒ…è£…
        use_cpu=True              # æ˜¾å¼å‘Šè¯‰ HF Trainer ä½¿ç”¨ CPU
    )

# =============================== 6. å¯åŠ¨è®­ç»ƒ =====================================
if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹å’Œ tokenizer
    model, tokenizer = load_model_and_tokenizer()
    
    # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†
    tokenized_dataset = load_and_preprocess_dataset(DATASET_PATH, tokenizer)
    
    # é…ç½® LoRA
    model = setup_lora(model)
    
    # æ•°æ®æ•´ç†å™¨ï¼ˆç”¨äºæ‰¹é‡å¤„ç†æ–‡æœ¬ï¼‰
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # å› æœLMä¸ä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡
    )
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=get_training_args(),
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["test"],
        tokenizer=tokenizer,
        data_collator=data_collator
    )

    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹ LoRA å¾®è°ƒ...")
    trainer.train()
    
    # ä¿å­˜æœ€ç»ˆ LoRA é€‚é…å™¨ï¼ˆä»…å‡  MBï¼Œæ— éœ€ä¿å­˜å®Œæ•´æ¨¡å‹ï¼‰
    trainer.save_model(f"{OUTPUT_DIR}/final_lora_adapter")
    print(f"âœ… è®­ç»ƒå®Œæˆï¼LoRA é€‚é…å™¨å·²ä¿å­˜åˆ° {OUTPUT_DIR}/final_lora_adapter")